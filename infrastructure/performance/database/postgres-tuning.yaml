# Open Forge PostgreSQL Performance Tuning Configuration
# Optimized settings for production workloads with pgvector
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: postgres-tuning-config
  namespace: open-forge
  labels:
    app.kubernetes.io/name: open-forge
    app.kubernetes.io/component: postgres
    app.kubernetes.io/part-of: open-forge
    app.kubernetes.io/managed-by: open-forge-performance
  annotations:
    description: "Production-optimized PostgreSQL configuration"
data:
  # Main PostgreSQL configuration
  postgresql.conf: |
    # =============================================================================
    # OPEN FORGE POSTGRESQL TUNING CONFIGURATION
    # Optimized for: 4 vCPU, 16GB RAM container
    # Adjust values based on your actual container resources
    # =============================================================================

    # =============================================================================
    # CONNECTION SETTINGS
    # =============================================================================

    # Maximum number of concurrent connections
    # Consider using PgBouncer for connection pooling
    max_connections = 200

    # Reserved connections for superuser
    superuser_reserved_connections = 3

    # Authentication timeout
    authentication_timeout = 1min

    # =============================================================================
    # MEMORY SETTINGS
    # =============================================================================

    # Shared memory for caching data
    # Recommended: 25% of total RAM
    # For 16GB: 4GB
    shared_buffers = 4GB

    # Memory for query operations (sorts, hashes, etc.)
    # Per-operation memory
    work_mem = 64MB

    # Memory for maintenance operations (VACUUM, CREATE INDEX, etc.)
    maintenance_work_mem = 1GB

    # Memory for autovacuum workers
    autovacuum_work_mem = 256MB

    # Effective cache size (OS + PostgreSQL cache)
    # Recommended: 75% of total RAM
    # For 16GB: 12GB
    effective_cache_size = 12GB

    # Huge pages (recommended for large shared_buffers)
    huge_pages = try

    # =============================================================================
    # WRITE-AHEAD LOG (WAL) SETTINGS
    # =============================================================================

    # WAL level for replication
    wal_level = replica

    # Maximum WAL size before checkpoint
    max_wal_size = 4GB

    # Minimum WAL size to retain
    min_wal_size = 1GB

    # WAL buffers (auto-tuned based on shared_buffers)
    wal_buffers = 64MB

    # Checkpoint completion target (0.0 - 1.0)
    # Higher value spreads I/O but increases recovery time
    checkpoint_completion_target = 0.9

    # Checkpoint timeout
    checkpoint_timeout = 15min

    # WAL compression (reduces I/O)
    wal_compression = on

    # =============================================================================
    # QUERY PLANNER SETTINGS
    # =============================================================================

    # Cost of sequential page fetch
    seq_page_cost = 1.0

    # Cost of random page fetch (lower for SSDs)
    random_page_cost = 1.1

    # Cost of processing each row
    cpu_tuple_cost = 0.01

    # Cost of processing each index entry
    cpu_index_tuple_cost = 0.005

    # Cost of processing each operator
    cpu_operator_cost = 0.0025

    # Effective I/O concurrency (higher for SSDs)
    effective_io_concurrency = 200

    # Number of parallel workers for gather operations
    max_parallel_workers_per_gather = 4

    # Total parallel workers
    max_parallel_workers = 8

    # Parallel maintenance workers
    max_parallel_maintenance_workers = 4

    # Minimum table size for parallel scan
    min_parallel_table_scan_size = 8MB

    # Minimum index size for parallel scan
    min_parallel_index_scan_size = 512kB

    # =============================================================================
    # BACKGROUND WRITER
    # =============================================================================

    # Delay between background writer runs
    bgwriter_delay = 200ms

    # Maximum buffers written per round
    bgwriter_lru_maxpages = 100

    # Fraction of buffers scanned per round
    bgwriter_lru_multiplier = 2.0

    # =============================================================================
    # AUTOVACUUM SETTINGS
    # =============================================================================

    # Enable autovacuum
    autovacuum = on

    # Maximum autovacuum workers
    autovacuum_max_workers = 4

    # Time between autovacuum runs
    autovacuum_naptime = 30s

    # Minimum dead tuples before vacuum
    autovacuum_vacuum_threshold = 50

    # Minimum changed tuples before analyze
    autovacuum_analyze_threshold = 50

    # Fraction of table to trigger vacuum
    autovacuum_vacuum_scale_factor = 0.05

    # Fraction of table to trigger analyze
    autovacuum_analyze_scale_factor = 0.025

    # Cost delay for autovacuum
    autovacuum_vacuum_cost_delay = 2ms

    # Cost limit for autovacuum
    autovacuum_vacuum_cost_limit = 1000

    # =============================================================================
    # STATEMENT TIMEOUTS
    # =============================================================================

    # Statement timeout (0 = disabled)
    # Set per-session for long-running queries
    statement_timeout = 0

    # Lock timeout
    lock_timeout = 30s

    # Idle transaction timeout
    idle_in_transaction_session_timeout = 10min

    # =============================================================================
    # LOGGING
    # =============================================================================

    # Log destination
    log_destination = 'stderr'

    # Enable CSV logging
    logging_collector = on
    log_directory = 'pg_log'
    log_filename = 'postgresql-%Y-%m-%d_%H%M%S.log'
    log_rotation_age = 1d
    log_rotation_size = 100MB

    # What to log
    log_min_duration_statement = 1000  # Log queries > 1 second
    log_checkpoints = on
    log_connections = on
    log_disconnections = on
    log_lock_waits = on
    log_temp_files = 0
    log_autovacuum_min_duration = 0

    # Log line format
    log_line_prefix = '%m [%p] %q%u@%d '

    # =============================================================================
    # STATISTICS
    # =============================================================================

    # Track function call statistics
    track_functions = all

    # Track I/O timing statistics
    track_io_timing = on

    # Statistics target (higher = more accurate, slower planning)
    default_statistics_target = 100

    # =============================================================================
    # pgvector EXTENSION SETTINGS
    # =============================================================================

    # Preload pgvector for better performance
    shared_preload_libraries = 'pg_stat_statements,pgvector'

    # pg_stat_statements settings
    pg_stat_statements.max = 10000
    pg_stat_statements.track = all

    # =============================================================================
    # REPLICATION (if using replicas)
    # =============================================================================

    # Maximum WAL senders
    max_wal_senders = 10

    # WAL keep size for replicas
    wal_keep_size = 1GB

    # Maximum replication slots
    max_replication_slots = 10

    # Synchronous commit for durability
    synchronous_commit = on

    # Hot standby for read replicas
    hot_standby = on

    # =============================================================================
    # SECURITY
    # =============================================================================

    # SSL mode
    ssl = on
    ssl_cert_file = '/etc/postgresql/server.crt'
    ssl_key_file = '/etc/postgresql/server.key'

    # Password encryption
    password_encryption = scram-sha-256

  # pg_hba.conf for authentication
  pg_hba.conf: |
    # TYPE  DATABASE        USER            ADDRESS                 METHOD
    local   all             all                                     scram-sha-256
    host    all             all             127.0.0.1/32            scram-sha-256
    host    all             all             ::1/128                 scram-sha-256
    host    all             all             10.0.0.0/8              scram-sha-256
    host    all             all             172.16.0.0/12           scram-sha-256
    host    all             all             192.168.0.0/16          scram-sha-256
    host    replication     replicator      10.0.0.0/8              scram-sha-256
    hostssl all             all             0.0.0.0/0               scram-sha-256
---
# Development Environment Configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: postgres-tuning-config-dev
  namespace: open-forge-dev
  labels:
    app.kubernetes.io/name: open-forge
    app.kubernetes.io/component: postgres
    environment: dev
data:
  postgresql.conf: |
    # Development Environment (2 vCPU, 4GB RAM)

    # Connection settings
    max_connections = 50
    superuser_reserved_connections = 3

    # Memory settings (for 4GB RAM)
    shared_buffers = 1GB
    work_mem = 32MB
    maintenance_work_mem = 256MB
    effective_cache_size = 3GB

    # WAL settings (relaxed for dev)
    max_wal_size = 1GB
    min_wal_size = 256MB
    checkpoint_completion_target = 0.7

    # Query planner
    random_page_cost = 1.1
    effective_io_concurrency = 100
    max_parallel_workers_per_gather = 2
    max_parallel_workers = 4

    # Autovacuum (more aggressive for dev)
    autovacuum = on
    autovacuum_max_workers = 2
    autovacuum_naptime = 1min

    # Logging (verbose for debugging)
    log_min_duration_statement = 100
    log_checkpoints = on
    log_lock_waits = on

    # Extensions
    shared_preload_libraries = 'pg_stat_statements,pgvector'
---
# Staging Environment Configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: postgres-tuning-config-staging
  namespace: open-forge-staging
  labels:
    app.kubernetes.io/name: open-forge
    app.kubernetes.io/component: postgres
    environment: staging
data:
  postgresql.conf: |
    # Staging Environment (2 vCPU, 8GB RAM)

    # Connection settings
    max_connections = 100
    superuser_reserved_connections = 3

    # Memory settings (for 8GB RAM)
    shared_buffers = 2GB
    work_mem = 48MB
    maintenance_work_mem = 512MB
    effective_cache_size = 6GB

    # WAL settings
    max_wal_size = 2GB
    min_wal_size = 512MB
    checkpoint_completion_target = 0.9

    # Query planner
    random_page_cost = 1.1
    effective_io_concurrency = 150
    max_parallel_workers_per_gather = 2
    max_parallel_workers = 4

    # Autovacuum
    autovacuum = on
    autovacuum_max_workers = 3
    autovacuum_naptime = 30s
    autovacuum_vacuum_scale_factor = 0.1
    autovacuum_analyze_scale_factor = 0.05

    # Logging
    log_min_duration_statement = 500
    log_checkpoints = on
    log_lock_waits = on

    # Timeouts
    statement_timeout = 300000  # 5 minutes
    idle_in_transaction_session_timeout = 300000

    # Extensions
    shared_preload_libraries = 'pg_stat_statements,pgvector'
---
# PostgreSQL Exporter ConfigMap for metrics
apiVersion: v1
kind: ConfigMap
metadata:
  name: postgres-exporter-queries
  namespace: open-forge
  labels:
    app.kubernetes.io/name: open-forge
    app.kubernetes.io/component: postgres
data:
  queries.yaml: |
    pg_replication:
      query: |
        SELECT
          CASE WHEN pg_is_in_recovery() THEN 1 ELSE 0 END AS is_replica,
          CASE WHEN pg_is_in_recovery() THEN
            EXTRACT(EPOCH FROM (now() - pg_last_xact_replay_timestamp()))
          ELSE 0 END AS lag_seconds
      metrics:
        - is_replica:
            usage: "GAUGE"
            description: "Whether the server is a replica"
        - lag_seconds:
            usage: "GAUGE"
            description: "Replication lag in seconds"

    pg_postmaster:
      query: |
        SELECT pg_postmaster_start_time as start_time_seconds
        FROM pg_postmaster_start_time()
      metrics:
        - start_time_seconds:
            usage: "GAUGE"
            description: "Time at which postmaster started"

    pg_database_size:
      query: |
        SELECT datname, pg_database_size(datname) as size_bytes
        FROM pg_database
        WHERE datistemplate = false
      metrics:
        - datname:
            usage: "LABEL"
            description: "Database name"
        - size_bytes:
            usage: "GAUGE"
            description: "Database size in bytes"

    pg_stat_user_tables:
      query: |
        SELECT
          schemaname,
          relname,
          seq_scan,
          seq_tup_read,
          idx_scan,
          idx_tup_fetch,
          n_tup_ins,
          n_tup_upd,
          n_tup_del,
          n_live_tup,
          n_dead_tup,
          last_vacuum,
          last_autovacuum,
          last_analyze,
          last_autoanalyze
        FROM pg_stat_user_tables
      metrics:
        - schemaname:
            usage: "LABEL"
            description: "Schema name"
        - relname:
            usage: "LABEL"
            description: "Table name"
        - seq_scan:
            usage: "COUNTER"
            description: "Sequential scans"
        - idx_scan:
            usage: "COUNTER"
            description: "Index scans"
        - n_live_tup:
            usage: "GAUGE"
            description: "Live tuples"
        - n_dead_tup:
            usage: "GAUGE"
            description: "Dead tuples"

    pg_locks:
      query: |
        SELECT
          pg_database.datname,
          tmp.mode,
          COALESCE(count, 0) as count
        FROM (
          SELECT database, mode, count(*) as count
          FROM pg_locks
          WHERE database IS NOT NULL
          GROUP BY database, mode
        ) AS tmp
        RIGHT JOIN pg_database ON pg_database.oid = tmp.database
      metrics:
        - datname:
            usage: "LABEL"
            description: "Database name"
        - mode:
            usage: "LABEL"
            description: "Lock mode"
        - count:
            usage: "GAUGE"
            description: "Number of locks"
